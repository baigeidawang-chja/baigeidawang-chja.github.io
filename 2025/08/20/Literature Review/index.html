<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="true" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>Literature Review | HAHA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning用于强化学习与模仿学习的可微轨迹优化 AbstractThis paper introduces DiffTORI, which utilizes Differentiable Trajectory Optim">
<meta property="og:type" content="article">
<meta property="og:title" content="Literature Review">
<meta property="og:url" content="http://example.com/2025/08/20/Literature%20Review/index.html">
<meta property="og:site_name" content="HAHA">
<meta property="og:description" content="DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning用于强化学习与模仿学习的可微轨迹优化 AbstractThis paper introduces DiffTORI, which utilizes Differentiable Trajectory Optim">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-20T07:58:47.949Z">
<meta property="article:modified_time" content="2025-08-20T12:38:56.763Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="HAHA" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  
  
    
<div id="banner" class="">
  <img src="/images/photo1.png" itemprop="image">
  <div id="banner-dim"></div>
</div>
 
   
  <div id="main-grid" class="  ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>HAHA </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/photo2.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">CHJA </div>
      <div class="dot"></div>
      <div class="subtitle">HAHA_DAWANG </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      



    
      

    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-Literature Review" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        Literature Review
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-08-20T07:58:47.949Z" itemprop="datePublished">2025-08-20</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
    未分类 
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            4.7k 词 
          </div>
        </div>
        
      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <hr>
<h3 id="DiffTORI-Differentiable-Trajectory-Optimization-for-Deep-Reinforcement-and-Imitation-Learning"><a href="#DiffTORI-Differentiable-Trajectory-Optimization-for-Deep-Reinforcement-and-Imitation-Learning" class="headerlink" title="DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning"></a>DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning</h3><h3 id="用于强化学习与模仿学习的可微轨迹优化"><a href="#用于强化学习与模仿学习的可微轨迹优化" class="headerlink" title="用于强化学习与模仿学习的可微轨迹优化"></a>用于强化学习与模仿学习的可微轨迹优化</h3><hr>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>This paper introduces DiffTORI, which utilizes Differentiable Trajectory Optimization as the policy representation to generate actions for deep Reinforcement and Imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTORI addresses the “objective mismatch” issue of prior model-based RL algorithms, as the dynamics model in DiffTORI is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTORI for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feed-forward policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15 model-based RL tasks and 35 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTORI outperforms prior state-of-the-art methods in both domains.</p>
<p>近期研究表明，策略表示方式对学习性能具有显著影响 [1; 2; 3; 4]。以往研究已探索将前馈神经网络 [4]、基于能量的模型 [2] 或扩散模型 [1; 5] 用作策略表示。在本文中，我们提出将可微轨迹优化 [3; 6; 7; 8; 9] 作为策略表示方式，为处理高维感官观测（图像 &#x2F; 点云）的深度强化学习（RL）和模仿学习（IL）生成动作。</p>
<p>轨迹优化是控制领域中一种高效且应用广泛的算法，由代价函数和动力学函数定义。它可被视为一种策略 [3; 6]，其中策略的参数用于确定代价函数和动力学函数。给定已学习的代价函数、动力学函数以及输入状态（例如图像、点云、机器人关节状态），该策略通过求解轨迹优化问题来计算动作。轨迹优化还可被设计为可微形式，这使得梯度能够通过轨迹优化过程进行反向传播 [3; 8; 10; 6; 9; 11; 12; 13]。在以往研究中，可微轨迹优化已被应用于系统辨识 [3; 6; 9]、逆最优控制 [6]、模仿学习 [3; 6; 8; 14; 7] 以及低维状态机器人问题的控制与规划 [3; 6; 8; 15]。</p>
<p>在本文中，我们提出将可微轨迹优化与基于深度模型的强化学习（RL）算法相结合。由于我们采用可微轨迹优化来生成动作 [10]，因此能够针对生成的动作计算策略梯度损失，进而学习动力学函数与代价函数以优化奖励。该方法解决了当前基于模型的 RL 算法存在的 “目标失配” 问题 [16; 17]—— 即动力学模型学习中训练性能更优（如均方误差（MSE）更低）的模型，未必更适用于控制任务。我们的方法之所以能解决这一问题，是因为通过轨迹优化过程反向传播策略梯度损失，潜动力学模型与奖励模型会共同朝着最大化任务性能的方向进行优化。实验表明，在 DeepMind 控制套件 [18] 包含的 15 项任务中（输入为高维图像），我们的方法性能优于以往的先进基于模型 RL 算法。</p>
<p>我们进一步在具有高维感官观测的标准机器人操作任务套件上对我们的模仿学习方法进行了基准测试，并将其与前馈策略类、基于能量的模型（EBM）[2]和扩散模型（Diffusion）[1]进行了比较，我们将自己的方法命名为DiffTORI（Differentiable Trajectory Optimization for Reinforcement and Imitation Learning，即可微轨迹优化的强化与模仿学习）。我们发现，使用可微轨迹优化的训练过程比先前工作中使用的EBM方法性能更优，后者由于需要采样高质量负例而可能面临训练不稳定性问题[1]。我们的方法还优于基于扩散的方法[1]，这得益于我们学习了一个代价函数，并在测试时对其进行优化。结果表明，DiffTORI在35项不同任务上均达到了最先进的性能：包括Robomimic[19]中的5项图像输入任务、Maniskill1[20]和Maniskill2[21]中的9项点云输入任务，以及MetaWorld[22]中的22项点云输入任务。</p>
<p>我们的研究与以往将可微轨迹优化用作策略表示的工作 [3; 8; 6] 密切相关。与这些先前研究相比，我们首次证明了可微轨迹优化如何与基于深度模型的强化学习（RL）算法相结合，并利用任务损失对动力学模型、奖励模型、Q 函数及策略进行端到端训练。相比之下，以往研究存在以下局限：要么仅聚焦于模仿学习 [3; 8]；要么假设动力学与奖励结构已知，仅学习少量参数 [3]；要么先通过动力学预测损失（而非任务损失）学习动力学模型，再将学到的固定动力学模型用于控制 [8]。此外，我们还首次证明了以可微轨迹优化为表示形式的策略类，能够扩展至图像、点云等高维感官观测场景，并在标准强化学习与模仿学习基准测试中实现最先进性能。而以往研究 [3; 8; 6] 仅在含真实低维状态的定制化任务中测试其方法，并未报告在含更复杂任务与高维观测的标准基准测试中的性能。</p>
<p>总而言之，本文的贡献如下：<br>1.我们提出了 DiffTORI 方法，该方法将可微轨迹优化用作深度强化学习（RL）与模仿学习（IL）的策略表示。<br>2.我们开展了大量实验，在含高维感官观测的标准基准测试中，将 DiffTORI 与以往的最先进方法进行对比 —— 其中基于模型的强化学习（RL）涉及 15 项任务，模仿学习（IL）涉及 35 项任务；实验结果表明，DiffTORI 的性能更优。<br>3.我们对 DiffTORI 进行了分析与消融实验，以揭示其性能提升的内在原因。</p>
<p><strong>2 相关工作</strong><br><strong>可微优化与隐式策略表示</strong>：本研究遵循可微优化领域的研究脉络，该领域将优化问题作为神经网络中的一层，以实现端到端学习。早期研究聚焦于对凸优化问题进行微分 [23; 24]。近期研究则拓展了可微优化问题的范围 [11; 12; 6; 8; 9; 10]。与本研究关联性最强的前期工作是 Amos 等人 [3] 和 Jin 等人 [6] 的研究，他们首次提出将轨迹优化视为隐式策略，并在行为克隆、系统辨识以及低维状态机器人控制场景中验证了该策略的有效性。另一项密切相关的近期工作是 Romero 等人 [15] 的研究，他们在 PPO 算法的智能体（actor）最后一层中嵌入了一个 “代价矩阵可学习、动力学已知” 的可微二次规划问题，并将其应用于四旋翼飞行器控制。本研究与之不同：我们学习的是由完整神经网络参数化的非线性代价函数，且会学习动力学函数（而非假设其已知）；同时，我们还验证了方法在图像、点云等高维感官输入场景中的有效性。Cheng 等人 [25; 26] 提出，在假设动力学已知的前提下，通过将控制器与系统动力学展开为计算图，并基于任务损失采用梯度下降法优化控制器参数，从而学习 PID 控制器的参数。而 DiffTORI 不假设任何关于动力学或策略类别的先验知识：我们并未将策略表示为预定义控制器，而是将其表示为 “利用以神经网络形式存在的已学习动力学、奖励及 Q 函数执行轨迹优化” 的过程。Sacks 等人 [27] 提出，在动力学与代价函数已知的前提下，通过强化学习学习 MPPI（模型预测路径积分）中的更新规则（以神经网络形式表示）。与之不同，本研究并非学习更新规则，而是学习轨迹优化中用于生成动作的动力学、奖励及 Q 函数；同时，我们采用可微轨迹优化（而非强化学习）来优化这些函数的参数。可微优化还被应用于其他机器人领域，如自动驾驶 [14; 28; 29]、导航 [7; 30]、运动规划 [31; 12] 及状态估计 [32]。本研究首次证明了可微轨迹优化如何与基于深度模型的强化学习（RL）相结合。</p>
<p><strong>基于模型的强化学习</strong>：与无模型强化学习（model-free RL）相比，基于模型的强化学习（model-based RL）通常具有更高的样本效率 —— 这是因为它在学习动力学模型时，本质上是在解决一个更简单的监督学习问题。近年来，研究者们发现了基于模型的强化学习存在一个基础性问题，即 “目标失配”[16]。近期研究提出了一种单一目标（该目标是策略真实回报的下界），用于在基于模型的强化学习中联合优化模型与策略 [17; 33]。我们的方法同样解决了目标失配问题。与上述两项仅优化 “真实回报下界” 的前期工作不同，我们的方法直接优化任务奖励。此外，这些方法仅在基于低维状态的观测场景中得到验证，而我们的方法能够处理高维图像或点云观测。同时，我们借助 Theseus [10] 通过解析方式计算真实目标的梯度，以此更新模型，这与上述工作也存在差异。另一项相关研究是 Nikishin 等人 [34] 的工作：他们在基于模型的强化学习中学习动力学模型与奖励模型，并将 “在已学习的动力学和奖励下，与最优 Q 函数相关联的 softmax 策略” 作为隐式策略；该策略通过隐函数定理反向传播强化学习损失来学习。与之不同，我们的隐式策略是 “利用已学习的动力学、奖励及 Q 函数执行轨迹优化后得到的最优解”。</p>
<p><strong>深度模仿学习的策略架构</strong>：模仿学习可被构建为一项监督回归任务，即从示范数据中学习 “观测到动作” 的映射关系。近期部分研究探索了不同的策略架构（如显式策略、隐式策略 [2]、扩散策略 [1]）与不同的动作表示形式（如高斯混合模型 [35; 19]、空间动作图 [36]、动作流 [4] 或参数化动作空间 [37]），旨在实现更精准的示范学习、对示范数据的多模态分布进行建模，并捕捉序列相关性。我们的方法之所以优于显式策略或扩散策略方法，是因为我们采用了 “学习代价函数，并在测试时对其进行优化” 的流程。而对于同样通过 “已学习目标 + 测试时优化” 实现的隐式策略，我们的差异在于：借助可微轨迹优化，采用了一种更稳定的全新训练流程。</p>
<p><strong>3 背景</strong></p>
<hr>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2025/08/21/Literature%20Review01/"
      title="Literature Riview01"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        Literature Riview01
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2025/08/20/hello-world/"
      title="Demo"
     >

    <p class="title-text">
      
        Demo
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>






    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2025 CHJA<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
