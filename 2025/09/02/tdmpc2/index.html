<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="true" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  

  
  <title>TD-mpc2 | HAHA</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="tdmpc2.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910">
<meta property="og:type" content="article">
<meta property="og:title" content="TD-mpc2">
<meta property="og:url" content="http://example.com/2025/09/02/tdmpc2/index.html">
<meta property="og:site_name" content="HAHA">
<meta property="og:description" content="tdmpc2.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-09-02T08:33:40.610Z">
<meta property="article:modified_time" content="2025-09-02T08:40:21.391Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="HAHA" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  
  
    
<div id="banner" class="">
  <img src="/images/photo1.png" itemprop="image">
  <div id="banner-dim"></div>
</div>
 
   
  <div id="main-grid" class="  ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>HAHA </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS 订阅">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS 订阅">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/photo2.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">CHJA </div>
      <div class="dot"></div>
      <div class="subtitle">HAHA_DAWANG </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      



    
      

    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-tdmpc2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        TD-mpc2
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-09-02T08:33:40.610Z" itemprop="datePublished">2025-09-02</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
    未分类 
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            13k 词 
          </div>
        </div>
        
      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
          <h2 id="tdmpc2-py"><a href="#tdmpc2-py" class="headerlink" title="tdmpc2.py"></a>tdmpc2.py</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">from common import math</span><br><span class="line">from common.scale import RunningScale</span><br><span class="line">from common.world_model import WorldModel</span><br><span class="line">from common.layers import api_model_conversion</span><br><span class="line">from tensordict import TensorDict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TDMPC2(torch.nn.Module):</span><br><span class="line">	<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">	TD-MPC2 agent. Implements training + inference.</span></span><br><span class="line"><span class="string">	Can be used for both single-task and multi-task experiments,</span></span><br><span class="line"><span class="string">	and supports both state and pixel observations.</span></span><br><span class="line"><span class="string">	&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">	def __init__(self, cfg):</span><br><span class="line">		super().__init__()</span><br><span class="line">		self.cfg = cfg</span><br><span class="line">		self.device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">		self.model = WorldModel(cfg).to(self.device)</span><br><span class="line">		self.optim = torch.optim.Adam([</span><br><span class="line">			&#123;<span class="string">&#x27;params&#x27;</span>: self.model._encoder.parameters(), <span class="string">&#x27;lr&#x27;</span>: self.cfg.lr*self.cfg.enc_lr_scale&#125;,</span><br><span class="line">			&#123;<span class="string">&#x27;params&#x27;</span>: self.model._dynamics.parameters()&#125;,</span><br><span class="line">			&#123;<span class="string">&#x27;params&#x27;</span>: self.model._reward.parameters()&#125;,</span><br><span class="line">			&#123;<span class="string">&#x27;params&#x27;</span>: self.model._termination.parameters() <span class="keyword">if</span> self.cfg.episodic <span class="keyword">else</span> []&#125;,</span><br><span class="line">			&#123;<span class="string">&#x27;params&#x27;</span>: self.model._Qs.parameters()&#125;,</span><br><span class="line">			&#123;<span class="string">&#x27;params&#x27;</span>: self.model._task_emb.parameters() <span class="keyword">if</span> self.cfg.multitask <span class="keyword">else</span> []</span><br><span class="line">			 &#125;</span><br><span class="line">		], lr=self.cfg.lr, capturable=True)</span><br><span class="line">		self.pi_optim = torch.optim.Adam(self.model._pi.parameters(), lr=self.cfg.lr, eps=1e-5, capturable=True)</span><br><span class="line">		self.model.eval()</span><br><span class="line">		self.scale = RunningScale(cfg)</span><br><span class="line">		self.cfg.iterations += 2*int(cfg.action_dim &gt;= 20) <span class="comment"># Heuristic for large action spaces</span></span><br><span class="line">		self.discount = torch.tensor(</span><br><span class="line">			[self._get_discount(ep_len) <span class="keyword">for</span> ep_len <span class="keyword">in</span> cfg.episode_lengths], device=<span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">		) <span class="keyword">if</span> self.cfg.multitask <span class="keyword">else</span> self._get_discount(cfg.episode_length)</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;Episode length:&#x27;</span>, cfg.episode_length)</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;Discount factor:&#x27;</span>, self.discount)</span><br><span class="line">		self._prev_mean = torch.nn.Buffer(torch.zeros(self.cfg.horizon, self.cfg.action_dim, device=self.device))</span><br><span class="line">		<span class="keyword">if</span> cfg.compile:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&#x27;Compiling update function with torch.compile...&#x27;</span>)</span><br><span class="line">			self._update = torch.compile(self._update, mode=<span class="string">&quot;reduce-overhead&quot;</span>)</span><br><span class="line"></span><br><span class="line">	@property</span><br><span class="line">	def plan(self):</span><br><span class="line">		_plan_val = getattr(self, <span class="string">&quot;_plan_val&quot;</span>, None)</span><br><span class="line">		<span class="keyword">if</span> _plan_val is not None:</span><br><span class="line">			<span class="built_in">return</span> _plan_val</span><br><span class="line">		<span class="keyword">if</span> self.cfg.compile:</span><br><span class="line">			plan = torch.compile(self._plan, mode=<span class="string">&quot;reduce-overhead&quot;</span>)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			plan = self._plan</span><br><span class="line">		self._plan_val = plan</span><br><span class="line">		<span class="built_in">return</span> self._plan_val</span><br><span class="line"></span><br><span class="line">	def _get_discount(self, episode_length):</span><br><span class="line">		<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">		Returns discount factor for a given episode length.</span></span><br><span class="line"><span class="string">		Simple heuristic that scales discount linearly with episode length.</span></span><br><span class="line"><span class="string">		Default values should work well for most tasks, but can be changed as needed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">		Args:</span></span><br><span class="line"><span class="string">			episode_length (int): Length of the episode. Assumes episodes are of fixed length.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">		Returns:</span></span><br><span class="line"><span class="string">			float: Discount factor for the task.</span></span><br><span class="line"><span class="string">		&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">		frac = episode_length/self.cfg.discount_denom</span><br><span class="line">		<span class="built_in">return</span> min(max((frac-<span class="number">1</span>)/(frac), self.cfg.discount_min), self.cfg.discount_max)</span><br><span class="line"></span><br><span class="line">	def save(self, fp):</span><br><span class="line">		&quot;&quot;&quot;</span><br><span class="line">		Save state dict of the agent to filepath.</span><br><span class="line"></span><br><span class="line">		Args:</span><br><span class="line">			fp (str): Filepath to save state dict to.</span><br><span class="line">		&quot;&quot;&quot;</span><br><span class="line">		torch.save(&#123;&quot;model&quot;: self.model.state_dict()&#125;, fp)</span><br><span class="line"></span><br><span class="line">	def load(self, fp):</span><br><span class="line">		&quot;&quot;&quot;</span><br><span class="line">		Load a saved state dict from filepath (or dictionary) into current agent.</span><br><span class="line"></span><br><span class="line">		Args:</span><br><span class="line">			fp (str or dict): Filepath or state dict to load.</span><br><span class="line">		&quot;&quot;&quot;</span><br><span class="line">		if isinstance(fp, dict):</span><br><span class="line">			state_dict = fp</span><br><span class="line">		else:</span><br><span class="line">			state_dict = torch.load(fp, map_location=torch.get_default_device(), weights_only=False)</span><br><span class="line">		state_dict = state_dict[&quot;model&quot;] if &quot;model&quot; in state_dict else state_dict</span><br><span class="line">		state_dict = api_model_conversion(self.model.state_dict(), state_dict)</span><br><span class="line">		self.model.load_state_dict(state_dict)</span><br><span class="line">		return</span><br><span class="line"></span><br><span class="line">	@torch.no_grad()</span><br><span class="line">	def act(self, obs, t0=False, eval_mode=False, task=None):</span><br><span class="line">		&quot;&quot;&quot;</span><br><span class="line">		Select an action by planning in the latent space of the world model.</span><br><span class="line"></span><br><span class="line">		Args:</span><br><span class="line">			obs (torch.Tensor): Observation from the environment.</span><br><span class="line">			t0 (bool): Whether this is the first observation in the episode.</span><br><span class="line">			eval_mode (bool): Whether to use the mean of the action distribution.</span><br><span class="line">			task (int): Task index (only used for multi-task experiments).</span><br><span class="line"></span><br><span class="line">		Returns:</span><br><span class="line">			torch.Tensor: Action to take in the environment.</span><br><span class="line">		&quot;&quot;&quot;</span><br><span class="line">		obs = obs.to(self.device, non_blocking=True).unsqueeze(<span class="number">0</span>)</span><br><span class="line">		if task is not None:</span><br><span class="line">			task = torch.tensor([task], device=self.device)</span><br><span class="line">		if self.cfg.mpc:</span><br><span class="line">			return self.plan(obs, t0=t0, eval_mode=eval_mode, task=task).cpu()</span><br><span class="line">		z = self.model.encode(obs, task)</span><br><span class="line">		action, info = self.model.pi(z, task)</span><br><span class="line">		if eval_mode:</span><br><span class="line">			action = info[&quot;mean&quot;]</span><br><span class="line">		return action[<span class="number">0</span>].cpu()</span><br><span class="line"></span><br><span class="line">	@torch.no_grad()</span><br><span class="line">	def _estimate_value(self, z, actions, task):</span><br><span class="line">		&quot;&quot;&quot;Estimate value of a trajectory starting at latent state z and executing given actions.&quot;&quot;&quot;</span><br><span class="line">		G, discount = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">		termination = torch.zeros(self.cfg.num_samples, <span class="number">1</span>, dtype=torch.float32, device=z.device)</span><br><span class="line">		for t in range(self.cfg.horizon):</span><br><span class="line">			reward = math.two_hot_inv(self.model.reward(z, actions[t], task), self.cfg)</span><br><span class="line">			z = self.model.next(z, actions[t], task)</span><br><span class="line">			G = G + discount * (<span class="number">1</span>-termination) * reward</span><br><span class="line">			discount_update = self.discount[torch.tensor(task)] if self.cfg.multitask else self.discount</span><br><span class="line">			discount = discount * discount_update</span><br><span class="line">			if self.cfg.episodic:</span><br><span class="line">				termination = torch.clip(termination + (self.model.termination(z, task) &gt; <span class="number">0.5</span>).float(), max=<span class="number">1</span>.)</span><br><span class="line">		action, _ = self.model.pi(z, task)</span><br><span class="line">		return G + discount * (<span class="number">1</span>-termination) * self.model.Q(z, action, task, return_type=&#x27;avg&#x27;)</span><br><span class="line"></span><br><span class="line">	@torch.no_grad()</span><br><span class="line">	def _plan(self, obs, t0=False, eval_mode=False, task=None):</span><br><span class="line">		&quot;&quot;&quot;</span><br><span class="line">		Plan a sequence of actions using the learned world model.</span><br><span class="line"></span><br><span class="line">		Args:</span><br><span class="line">			z (torch.Tensor): Latent state from which to plan.</span><br><span class="line">			t0 (bool): Whether this is the first observation in the episode.</span><br><span class="line">			eval_mode (bool): Whether to use the mean of the action distribution.</span><br><span class="line">			task (Torch.Tensor): Task index (only used for multi-task experiments).</span><br><span class="line"></span><br><span class="line">		Returns:</span><br><span class="line">			torch.Tensor: Action to take in the environment.</span><br><span class="line">		&quot;&quot;&quot;</span><br><span class="line">		# Sample policy trajectories</span><br><span class="line">		z = self.model.encode(obs, task)</span><br><span class="line">		if self.cfg.num_pi_trajs &gt; <span class="number">0</span>:</span><br><span class="line">			pi_actions = torch.empty(self.cfg.horizon, self.cfg.num_pi_trajs, self.cfg.action_dim, device=self.device)</span><br><span class="line">			_z = z.repeat(self.cfg.num_pi_trajs, <span class="number">1</span>)</span><br><span class="line">			for t in range(self.cfg.horizon-<span class="number">1</span>):</span><br><span class="line">				pi_actions[t], _ = self.model.pi(_z, task)</span><br><span class="line">				_z = self.model.next(_z, pi_actions[t], task)</span><br><span class="line">			pi_actions[-<span class="number">1</span>], _ = self.model.pi(_z, task)</span><br><span class="line"></span><br><span class="line">		# Initialize state and parameters</span><br><span class="line">		z = z.repeat(self.cfg.num_samples, <span class="number">1</span>)</span><br><span class="line">		mean = torch.zeros(self.cfg.horizon, self.cfg.action_dim, device=self.device)</span><br><span class="line">		std = torch.full((self.cfg.horizon, self.cfg.action_dim), self.cfg.max_std, dtype=torch.float, device=self.device)</span><br><span class="line">		if not t0:</span><br><span class="line">			mean[:-<span class="number">1</span>] = self._prev_mean[<span class="number">1</span>:]</span><br><span class="line">		actions = torch.empty(self.cfg.horizon, self.cfg.num_samples, self.cfg.action_dim, device=self.device)</span><br><span class="line">		if self.cfg.num_pi_trajs &gt; <span class="number">0</span>:</span><br><span class="line">			actions[:, :self.cfg.num_pi_trajs] = pi_actions</span><br><span class="line"></span><br><span class="line">		# Iterate MPPI</span><br><span class="line">		for _ in range(self.cfg.iterations):</span><br><span class="line"></span><br><span class="line">			# Sample actions</span><br><span class="line">			r = torch.randn(self.cfg.horizon, self.cfg.num_samples-self.cfg.num_pi_trajs, self.cfg.action_dim, device=std.device)</span><br><span class="line">			actions_sample = mean.unsqueeze(<span class="number">1</span>) + std.unsqueeze(<span class="number">1</span>) * r</span><br><span class="line">			actions_sample = actions_sample.clamp(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">			actions[:, self.cfg.num_pi_trajs:] = actions_sample</span><br><span class="line">			if self.cfg.multitask:</span><br><span class="line">				actions = actions * self.model._action_masks[task]</span><br><span class="line"></span><br><span class="line">			# Compute elite actions</span><br><span class="line">			value = self._estimate_value(z, actions, task).nan_to_num(<span class="number">0</span>)</span><br><span class="line">			elite_idxs = torch.topk(value.squeeze(<span class="number">1</span>), self.cfg.num_elites, dim=<span class="number">0</span>).indices</span><br><span class="line">			elite_value, elite_actions = value[elite_idxs], actions[:, elite_idxs]</span><br><span class="line"></span><br><span class="line">			# Update parameters</span><br><span class="line">			max_value = elite_value.max(<span class="number">0</span>).values</span><br><span class="line">			score = torch.exp(self.cfg.temperature*(elite_value - max_value))</span><br><span class="line">			score = score / score.sum(0)</span><br><span class="line">			mean = (score.unsqueeze(0) * elite_actions).<span class="built_in">sum</span>(dim=1) / (score.sum(0) + 1e-9)</span><br><span class="line">			std = ((score.unsqueeze(<span class="number">0</span>) * (elite_actions - mean.unsqueeze(<span class="number">1</span>)) ** 2).<span class="built_in">sum</span>(dim=1) / (score.sum(0) + 1e-9)).sqrt()</span><br><span class="line">			std = std.clamp(self.cfg.min_std, self.cfg.max_std)</span><br><span class="line">			<span class="keyword">if</span> self.cfg.multitask:</span><br><span class="line">				mean = mean * self.model._action_masks[task]</span><br><span class="line">				std = std * self.model._action_masks[task]</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Select action</span></span><br><span class="line">		rand_idx = math.gumbel_softmax_sample(score.squeeze(1))</span><br><span class="line">		actions = torch.index_select(elite_actions, 1, rand_idx).squeeze(1)</span><br><span class="line">		a, std = actions[0], std[0]</span><br><span class="line">		<span class="keyword">if</span> not eval_mode:</span><br><span class="line">			a = a + std * torch.randn(self.cfg.action_dim, device=std.device)</span><br><span class="line">		self._prev_mean.copy_(mean)</span><br><span class="line">		<span class="built_in">return</span> a.clamp(-1, 1)</span><br><span class="line"></span><br><span class="line">	def update_pi(self, zs, task):</span><br><span class="line">		<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">		Update policy using a sequence of latent states.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">		Args:</span></span><br><span class="line"><span class="string">			zs (torch.Tensor): Sequence of latent states.</span></span><br><span class="line"><span class="string">			task (torch.Tensor): Task index (only used for multi-task experiments).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">		Returns:</span></span><br><span class="line"><span class="string">			float: Loss of the policy update.</span></span><br><span class="line"><span class="string">		&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">		action, info = self.model.pi(zs, task)</span><br><span class="line">		qs = self.model.Q(zs, action, task, return_type=<span class="string">&#x27;avg&#x27;</span>, detach=True)</span><br><span class="line">		self.scale.update(qs[0])</span><br><span class="line">		qs = self.scale(qs)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Loss is a weighted sum of Q-values</span></span><br><span class="line">		rho = torch.pow(self.cfg.rho, torch.arange(len(qs), device=self.device))</span><br><span class="line">		pi_loss = (-(self.cfg.entropy_coef * info[<span class="string">&quot;scaled_entropy&quot;</span>] + qs).mean(dim=(1,2)) * rho).mean()</span><br><span class="line">		pi_loss.backward()</span><br><span class="line">		pi_grad_norm = torch.nn.utils.clip_grad_norm_(self.model._pi.parameters(), self.cfg.grad_clip_norm)</span><br><span class="line">		self.pi_optim.step()</span><br><span class="line">		self.pi_optim.zero_grad(set_to_none=True)</span><br><span class="line"></span><br><span class="line">		info = TensorDict(&#123;</span><br><span class="line">			<span class="string">&quot;pi_loss&quot;</span>: pi_loss,</span><br><span class="line">			<span class="string">&quot;pi_grad_norm&quot;</span>: pi_grad_norm,</span><br><span class="line">			<span class="string">&quot;pi_entropy&quot;</span>: info[<span class="string">&quot;entropy&quot;</span>],</span><br><span class="line">			<span class="string">&quot;pi_scaled_entropy&quot;</span>: info[<span class="string">&quot;scaled_entropy&quot;</span>],</span><br><span class="line">			<span class="string">&quot;pi_scale&quot;</span>: self.scale.value,</span><br><span class="line">		&#125;)</span><br><span class="line">		<span class="built_in">return</span> info</span><br><span class="line"></span><br><span class="line">	@torch.no_grad()</span><br><span class="line">	def _td_target(self, next_z, reward, terminated, task):</span><br><span class="line">		<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">		Compute the TD-target from a reward and the observation at the following time step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">		Args:</span></span><br><span class="line"><span class="string">			next_z (torch.Tensor): Latent state at the following time step.</span></span><br><span class="line"><span class="string">			reward (torch.Tensor): Reward at the current time step.</span></span><br><span class="line"><span class="string">			terminated (torch.Tensor): Termination signal at the current time step.</span></span><br><span class="line"><span class="string">			task (torch.Tensor): Task index (only used for multi-task experiments).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">		Returns:</span></span><br><span class="line"><span class="string">			torch.Tensor: TD-target.</span></span><br><span class="line"><span class="string">		&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">		action, _ = self.model.pi(next_z, task)</span><br><span class="line">		discount = self.discount[task].unsqueeze(-1) <span class="keyword">if</span> self.cfg.multitask <span class="keyword">else</span> self.discount</span><br><span class="line">		<span class="built_in">return</span> reward + discount * (1-terminated) * self.model.Q(next_z, action, task, return_type=<span class="string">&#x27;min&#x27;</span>, target=True)</span><br><span class="line"></span><br><span class="line">	def _update(self, obs, action, reward, terminated, task=None):</span><br><span class="line">		<span class="comment"># Compute targets</span></span><br><span class="line">		with torch.no_grad():</span><br><span class="line">			next_z = self.model.encode(obs[1:], task)</span><br><span class="line">			td_targets = self._td_target(next_z, reward, terminated, task)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Prepare for update</span></span><br><span class="line">		self.model.train()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Latent rollout</span></span><br><span class="line">		zs = torch.empty(self.cfg.horizon+1, self.cfg.batch_size, self.cfg.latent_dim, device=self.device)</span><br><span class="line">		z = self.model.encode(obs[0], task)</span><br><span class="line">		zs[0] = z</span><br><span class="line">		consistency_loss = 0</span><br><span class="line">		<span class="keyword">for</span> t, (_action, _next_z) <span class="keyword">in</span> enumerate(zip(action.unbind(0), next_z.unbind(0))):</span><br><span class="line">			z = self.model.next(z, _action, task)</span><br><span class="line">			consistency_loss = consistency_loss + F.mse_loss(z, _next_z) * self.cfg.rho**t</span><br><span class="line">			zs[t+1] = z</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Predictions</span></span><br><span class="line">		_zs = zs[:-1]</span><br><span class="line">		qs = self.model.Q(_zs, action, task, return_type=<span class="string">&#x27;all&#x27;</span>)</span><br><span class="line">		reward_preds = self.model.reward(_zs, action, task)</span><br><span class="line">		<span class="keyword">if</span> self.cfg.episodic:</span><br><span class="line">			termination_pred = self.model.termination(zs[1:], task, unnormalized=True)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Compute losses</span></span><br><span class="line">		reward_loss, value_loss = 0, 0</span><br><span class="line">		<span class="keyword">for</span> t, (rew_pred_unbind, rew_unbind, td_targets_unbind, qs_unbind) <span class="keyword">in</span> enumerate(zip(reward_preds.unbind(0), reward.unbind(0), td_targets.unbind(0), qs.unbind(1))):</span><br><span class="line">			reward_loss = reward_loss + math.soft_ce(rew_pred_unbind, rew_unbind, self.cfg).mean() * self.cfg.rho**t</span><br><span class="line">			<span class="keyword">for</span> _, qs_unbind_unbind <span class="keyword">in</span> enumerate(qs_unbind.unbind(0)):</span><br><span class="line">				value_loss = value_loss + math.soft_ce(qs_unbind_unbind, td_targets_unbind, self.cfg).mean() * self.cfg.rho**t</span><br><span class="line"></span><br><span class="line">		consistency_loss = consistency_loss / self.cfg.horizon</span><br><span class="line">		reward_loss = reward_loss / self.cfg.horizon</span><br><span class="line">		<span class="keyword">if</span> self.cfg.episodic:</span><br><span class="line">			termination_loss = F.binary_cross_entropy_with_logits(termination_pred, terminated)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			termination_loss = 0.</span><br><span class="line">		value_loss = value_loss / (self.cfg.horizon * self.cfg.num_q)</span><br><span class="line">		total_loss = (</span><br><span class="line">			self.cfg.consistency_coef * consistency_loss +</span><br><span class="line">			self.cfg.reward_coef * reward_loss +</span><br><span class="line">			self.cfg.termination_coef * termination_loss +</span><br><span class="line">			self.cfg.value_coef * value_loss</span><br><span class="line">		)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Update model</span></span><br><span class="line">		total_loss.backward()</span><br><span class="line">		grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip_norm)</span><br><span class="line">		self.optim.step()</span><br><span class="line">		self.optim.zero_grad(set_to_none=True)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Update policy</span></span><br><span class="line">		pi_info = self.update_pi(zs.detach(), task)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Update target Q-functions</span></span><br><span class="line">		self.model.soft_update_target_Q()</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Return training statistics</span></span><br><span class="line">		self.model.eval()</span><br><span class="line">		info = TensorDict(&#123;</span><br><span class="line">			<span class="string">&quot;consistency_loss&quot;</span>: consistency_loss,</span><br><span class="line">			<span class="string">&quot;reward_loss&quot;</span>: reward_loss,</span><br><span class="line">			<span class="string">&quot;value_loss&quot;</span>: value_loss,</span><br><span class="line">			<span class="string">&quot;termination_loss&quot;</span>: termination_loss,</span><br><span class="line">			<span class="string">&quot;total_loss&quot;</span>: total_loss,</span><br><span class="line">			<span class="string">&quot;grad_norm&quot;</span>: grad_norm,</span><br><span class="line">		&#125;)</span><br><span class="line">		<span class="keyword">if</span> self.cfg.episodic:</span><br><span class="line">			info.update(math.termination_statistics(torch.sigmoid(termination_pred[-1]), terminated[-1]))</span><br><span class="line">		info.update(pi_info)</span><br><span class="line">		<span class="built_in">return</span> info.detach().mean()</span><br><span class="line"></span><br><span class="line">	def update(self, buffer):</span><br><span class="line">		<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">		Main update function. Corresponds to one iteration of model learning.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">		Args:</span></span><br><span class="line"><span class="string">			buffer (common.buffer.Buffer): Replay buffer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">		Returns:</span></span><br><span class="line"><span class="string">			dict: Dictionary of training statistics.</span></span><br><span class="line"><span class="string">		&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">		obs, action, reward, terminated, task = buffer.sample()</span><br><span class="line">		kwargs = &#123;&#125;</span><br><span class="line">		<span class="keyword">if</span> task is not None:</span><br><span class="line">			kwargs[<span class="string">&quot;task&quot;</span>] = task</span><br><span class="line">		torch.compiler.cudagraph_mark_step_begin()</span><br><span class="line">		<span class="built_in">return</span> self._update(obs, action, reward, terminated, **kwargs)</span><br></pre></td></tr></table></figure>
        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2025/09/03/gymnasium/"
      title="gymnasium"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        gymnasium
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2025/08/21/Literature%20Review01/"
      title="Literature Riview01"
     >

    <p class="title-text">
      
        Literature Riview01
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>






    </div>
    <div id="footer-wrapper">
      <footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2026 CHJA<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
